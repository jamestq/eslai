{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b141f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e60fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_accent_map(df):\n",
    "    accent_counts = df[\"accents\"].value_counts()\n",
    "    accent_types = accent_counts[0:40].index.tolist()\n",
    "    accent_map_dict = {\n",
    "        \"accent\": accent_types,\n",
    "        \"reduced_accent\": [\"\"]*len(accent_types),\n",
    "    }\n",
    "    accent_map = pd.DataFrame(accent_map_dict)\n",
    "    accent_map.to_csv(\"unprocessed_accent_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36fcba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/90h7g4l56pqcr9bp0_c4cfkw0000gn/T/ipykernel_19422/2277223982.py:2: DtypeWarning: Columns (4,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  validated = pd.read_csv(\"../raw/validated.tsv\", sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accents\n",
      "United States English                                438469\n",
      "England English                                      154750\n",
      "India and South Asia (India, Pakistan, Sri Lanka)    109144\n",
      "Canadian English                                      75390\n",
      "Australian English                                    55016\n",
      "                                                      ...  \n",
      "Turkmen                                                   1\n",
      "United States English,British,iowa                        1\n",
      "Chinese accent of English                                 1\n",
      "Southern counties English                                 1\n",
      "South Asia (Bangladesh)                                   1\n",
      "Name: count, Length: 771, dtype: int64\n",
      "reduced_accents\n",
      "North American       523284\n",
      "British              162814\n",
      "South Asian          109554\n",
      "ANZ                   69325\n",
      "European              53232\n",
      "South African         26613\n",
      "Irish                 21127\n",
      "Scottish              18626\n",
      "Filipino               6461\n",
      "SEA                    5919\n",
      "Chinese                4706\n",
      "Jamaican                852\n",
      "Northern American       493\n",
      "African                 356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "accent_map = pd.read_csv(\"data/accent_map.csv\")\n",
    "validated = pd.read_csv(\"../raw/validated.tsv\", sep=\"\\t\")\n",
    "validated = validated[[\"client_id\", \"sentence_id\", \"path\", \"sentence\", \"age\", \"gender\", \"accents\"]]\n",
    "print(validated[\"accents\"].value_counts())\n",
    "validated_reduced = validated[[\"client_id\", \"sentence_id\", \"path\", \"sentence\", \"age\", \"gender\"]].copy()\n",
    "validated_reduced[\"reduced_accents\"] = validated[\"accents\"].map(accent_map.set_index(\"accent\")[\"reduced_accent\"])\n",
    "print(validated_reduced[\"reduced_accents\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9fc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/available_audio.txt\", \"r\") as f:\n",
    "    available_audio = pd.Series(f.read().splitlines())\n",
    "    validated_reduced = validated_reduced[validated_reduced[\"path\"].isin(available_audio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce347c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/90h7g4l56pqcr9bp0_c4cfkw0000gn/T/ipykernel_19422/3064510566.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validated_accent_train = validated_reduced.groupby(\"reduced_accents\").apply(randomise_datapoints, include_groups=True).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reduced_accents\n",
       "ANZ                  300\n",
       "African              300\n",
       "British              300\n",
       "Chinese              300\n",
       "European             300\n",
       "Filipino             300\n",
       "Irish                300\n",
       "Jamaican             300\n",
       "North American       300\n",
       "Northern American    300\n",
       "SEA                  300\n",
       "Scottish             300\n",
       "South African        300\n",
       "South Asian          300\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def randomise_datapoints(x):\n",
    "    \"\"\"Get 300 data points for each accent.\"\"\"\n",
    "    if len(x) < 300:\n",
    "        return x\n",
    "    else:\n",
    "        return x.sample(n=300, random_state=42)\n",
    "# Get 300 data points for each accent\n",
    "validated_accent_train = validated_reduced.groupby(\"reduced_accents\").apply(randomise_datapoints, include_groups=True).reset_index(drop=True)\n",
    "validated_accent_train[\"reduced_accents\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed43d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/90h7g4l56pqcr9bp0_c4cfkw0000gn/T/ipykernel_19422/2616287192.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validated_accent_test = validated_reduced_subset.groupby(\"reduced_accents\").apply(randomise_datapoints, include_groups=True).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reduced_accents\n",
       "ANZ                  300\n",
       "British              300\n",
       "Chinese              300\n",
       "European             300\n",
       "Filipino             300\n",
       "Irish                300\n",
       "North American       300\n",
       "SEA                  300\n",
       "Scottish             300\n",
       "South African        300\n",
       "South Asian          300\n",
       "Northern American     83\n",
       "African               43\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_reduced_subset = validated_reduced[~validated_reduced[\"path\"].isin(validated_accent_train[\"path\"])] \n",
    "validated_accent_test = validated_reduced_subset.groupby(\"reduced_accents\").apply(randomise_datapoints, include_groups=True).reset_index(drop=True)\n",
    "validated_accent_test[\"reduced_accents\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7150890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "stem  = Path(\"data\")\n",
    "audio_data_train_path = stem / \"audio_data_train.txt\"\n",
    "audio_data_test_path = stem / \"audio_data_test.txt\"\n",
    "accent_train_path = stem / \"accent_train.tsv\"\n",
    "accent_test_path = stem / \"accent_test.tsv\"\n",
    "\n",
    "audio_data_train = \"\\n\".join(validated_accent_train[\"path\"].tolist())\n",
    "audio_data_train_path.write_text(audio_data_train)\n",
    "audio_data_test = \"\\n\".join(validated_accent_test[\"path\"].tolist())\n",
    "audio_data_test_path.write_text(audio_data_test)\n",
    "validated_accent_train.to_csv(accent_train_path, index=False, sep=\"\\t\")\n",
    "validated_accent_test.to_csv(accent_test_path, index=False, sep=\"\\t\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
